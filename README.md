# Maze-Solving-Using-QLearning
The Q-learning algorithm in this code is used to find the shortest path in a maze by iteratively learning the optimal actions to take from each state. The algorithm starts by initializing the Q-table with zero values for all possible actions (up, down, left, right) at each open cell in the maze. The learning process runs for a specified number of episodes (‘self.episodes’ set to 1000), where in each episode, the agent starts from the initial state ‘(0,0)’ and tries to reach the goal state ‘(rows-1, cols-1)’ within a maximum number of steps (‘self.max_steps_per_episode’ set to 100). During each episode, the agent chooses actions based on an epsilon-greedy policy (‘self.exp’ set to 0.1), where it either explores by picking a random action or exploits by choosing the best-known action according to the Q- table. The agent then moves to the next state using the ‘findNextState’ function and receives a reward using the ‘calculateReward’ function based on the new state's validity and proximity to the goal. Rewards are -100 for hitting a wall or going out of bounds, 100 for reaching the goal, and -1 for valid moves. The Q-value for the current state- action pair is updated using the formula: [ Q(s,a) = Q(s,a) + alpha * (r + gamma * max(Q(s',a')) - Q(s,a)) ] where alpha - learning rate is set to 0.1, gamma - discount factor is set to 0.9, r is the reward, and max(Q(s',a')) is the maximum Q-value for the next state. This update process helps the agent to learn the optimal policy over time.

After training, the agent uses the learned Q-values to find the optimal path from the initial state to the goal state. The discovered path is marked in the maze by replacing 0s with 2s. Finally, the maze with the discovered path is written to an output file. If no path is found, an appropriate message is displayed. This Q-learning approach allows the agent to autonomously learn and navigate the maze by balancing exploration and exploitation, iteratively improving its policy to find the shortest path.
